{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training ViT Model","metadata":{}},{"cell_type":"code","source":"import zipfile\nimport os\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import vit_b_16\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\ndata_dir = \"/kaggle/input/imagedataset\"\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\ntrain_data = datasets.ImageFolder(root=f\"{data_dir}/train\", transform=transform)\nval_data = datasets.ImageFolder(root=f\"{data_dir}/validation\", transform=transform)\ntest_data = datasets.ImageFolder(root=f\"{data_dir}/test\", transform=transform)\n\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=32)\ntest_loader = DataLoader(test_data, batch_size=32)\n\n# Model Setup (Vision Transformer)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = vit_b_16(pretrained=True)\nmodel.heads.head = nn.Linear(model.heads.head.in_features, 2)  # binary classification\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Training Loop\ndef train_model(model, loader, val_loader, epochs=5):\n    train_losses, val_losses = [], []\n    train_accuracies, val_accuracies = [], []\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            output = model(images)\n            loss = criterion(output, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n            _, predicted = torch.max(output, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n        train_acc = 100 * correct / total\n        train_epoch_loss = total_loss / len(loader)\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                val_correct += (predicted == labels).sum().item()\n                val_total += labels.size(0)\n\n        val_acc = 100 * val_correct / val_total\n        val_epoch_loss = val_loss / len(val_loader)\n\n        train_losses.append(train_epoch_loss)\n        val_losses.append(val_epoch_loss)\n        train_accuracies.append(train_acc)\n        val_accuracies.append(val_acc)\n\n        print(f\"Epoch [{epoch+1}/{epochs}] Train Loss: {train_epoch_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_epoch_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n\n    return train_losses, val_losses, train_accuracies, val_accuracies\n\n# Train the Model\ntrain_losses, val_losses, train_accuracies, val_accuracies = train_model(model, train_loader, val_loader)\n\n# Save the model\nmodel_save_path = \"/kaggle/working/vit_model.pth\"\ntorch.save(model, model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n\n# Plot evaluation metrics\ndef plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies):\n    epochs = range(1, len(train_losses) + 1)\n\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, label='Train Loss')\n    plt.plot(epochs, val_losses, label='Val Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Loss Curves')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_accuracies, label='Train Acc')\n    plt.plot(epochs, val_accuracies, label='Val Acc')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Accuracy Curves')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.savefig('/kaggle/working/training_curves.png')  # Optional: Save plot\n    plt.show()\n\ndef evaluate_model(model, val_loader, device=None):\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n\n    with torch.no_grad():\n        for x, y in val_loader:\n            x, y = x.to(device), y.to(device)\n            outputs = model(x)\n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.argmax(probs, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(y.cpu().numpy())\n            all_probs.extend(probs[:, 1].cpu().numpy())\n\n    # Confusion Matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.title('Confusion Matrix')\n    plt.tight_layout()\n    plt.savefig('/kaggle/working/confusion_matrix.png')  # Optional: Save plot\n    plt.show()\n\n    # Classification Report\n    print(\"Classification Report:\\n\")\n    print(classification_report(all_labels, all_preds, target_names=[\"Real\", \"Fake\"]))\n\n    # ROC Curve\n    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure(figsize=(6, 5))\n    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc='lower right')\n    plt.grid()\n    plt.tight_layout()\n    plt.savefig('/kaggle/working/roc_curve.png')  # Optional: Save plot\n    plt.show()\n\n# Visualize Training and Evaluate\nplot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies)\nevaluate_model(model, val_loader)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training XceptionNet+GRU model (on .mp4 data)","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\n# Customise the video dataset\nclass VideoDataset(Dataset):\n    def __init__(self, video_dir, transform, max_frames=16):\n        self.samples = []\n        self.transform = transform\n        self.max_frames = max_frames\n        for label, subdir in enumerate(['real', 'fake']):\n            folder = os.path.join(video_dir, subdir)\n            for file in os.listdir(folder):\n                if file.endswith(\".mp4\"):\n                    self.samples.append((os.path.join(folder, file), label))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        path, label = self.samples[idx]\n        cap = cv2.VideoCapture(path)\n        frames = []\n        count = 0\n        while count < self.max_frames and cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame = Image.fromarray(frame)\n            frames.append(self.transform(frame))\n            count += 1\n        cap.release()\n        while len(frames) < self.max_frames:\n            frames.append(frames[-1])\n        return torch.stack(frames), label\n\n# Image transforms\nvideo_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Load dataset\nvideo_ds = VideoDataset(\"/content/datasets\", transform=video_transform)\ntrain_size = int(0.8 * len(video_ds))\nval_size = len(video_ds) - train_size\ntrain_ds, val_ds = torch.utils.data.random_split(video_ds, [train_size, val_size])\ntrain_dl = DataLoader(train_ds, batch_size=4, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=4)\n\n# Model: EfficientNet + GRU\nclass XceptionGRU(nn.Module):\n    def __init__(self, hidden_size=256, num_classes=2):\n        super().__init__()\n        self.cnn = models.efficientnet_b0(pretrained=True)\n        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])\n        for param in self.cnn.parameters():\n            param.requires_grad = False\n        self.gru = nn.GRU(input_size=1280, hidden_size=hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        B, T, C, H, W = x.shape\n        x = x.view(B * T, C, H, W)\n        with torch.no_grad():\n            x = self.cnn(x)\n        x = x.view(B, T, -1)\n        _, h = self.gru(x)\n        return self.fc(h[-1])\n\n# Device and model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = XceptionGRU().to(device)\n\n# Train + Eval Functions\ndef train_model(model, train_dl, val_dl, epochs=10, lr=0.001):\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    train_losses, val_losses = [], []\n    train_accuracies, val_accuracies = [], []\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss, correct, total = 0, 0, 0\n        for inputs, labels in train_dl:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n        train_losses.append(total_loss / len(train_dl))\n        train_accuracies.append(correct / total)\n\n        # Validation\n        model.eval()\n        val_loss, val_correct, val_total = 0, 0, 0\n        with torch.no_grad():\n            for inputs, labels in val_dl:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, preds = torch.max(outputs, 1)\n                val_correct += (preds == labels).sum().item()\n                val_total += labels.size(0)\n        val_losses.append(val_loss / len(val_dl))\n        val_accuracies.append(val_correct / val_total)\n\n        print(f\"Epoch [{epoch+1}/{epochs}] \"\n              f\"Train Loss: {train_losses[-1]:.4f}, Acc: {train_accuracies[-1]*100:.2f}% | \"\n              f\"Val Loss: {val_losses[-1]:.4f}, Acc: {val_accuracies[-1]*100:.2f}%\")\n\n    return train_losses, val_losses, train_accuracies, val_accuracies\n\n# Plotting curves\ndef plot_training_curves(train_losses, val_losses, train_acc, val_acc):\n    epochs = range(1, len(train_losses) + 1)\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n    plt.plot(epochs, val_losses, 'r-', label='Val Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Loss Curve')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_acc, 'b-', label='Train Accuracy')\n    plt.plot(epochs, val_acc, 'r-', label='Val Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.title('Accuracy Curve')\n\n    plt.tight_layout()\n    plt.show()\n\n# Evaluate the model\ndef evaluate_model(model, dataloader):\n    model.eval()\n    all_labels = []\n    all_preds = []\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n\n    print(\"ðŸ“Š Classification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=[\"Real\", \"Fake\"]))\n\n    cm = confusion_matrix(all_labels, all_preds)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Real\", \"Fake\"])\n    disp.plot(cmap='Blues')\n    plt.show()\n\n# Train and Save\ntrain_losses, val_losses, train_acc, val_acc = train_model(model, train_dl, val_dl)\nplot_training_curves(train_losses, val_losses, train_acc, val_acc)\nevaluate_model(model, val_dl)\n\ntorch.save(model.state_dict(), \"/content/drive/MyDrive/DeepFake/xception_gru_video_model.pth\")\nprint(\"Model Saved Successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training XceptionNet+GRU model (on sequential images data)","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport os\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport timm\n\n# Unzip Dataset\n!unzip -q \"/content/drive/MyDrive/DeepFake/SeqImagesDataset.zip\" -d /content/\n\n# Dataset Path\nseq_data_path = \"/content\"\n\n# Custom Dataset Loader (for Sequences of Images)\nclass SeqImageDataset(Dataset):\n    def __init__(self, root_dir, transform=None, sequence_len=10):\n        self.sequence_len = sequence_len\n        self.data = []\n        self.labels = []\n        self.transform = transform\n\n        for label_idx, label in enumerate(['real', 'fake']):\n            folder = os.path.join(root_dir, label)\n            if not os.path.exists(folder):\n                print(f\"Warning: Folder {folder} does not exist!\")\n                continue\n\n            videos = {}\n            for file in sorted(os.listdir(folder)):\n                video_id = \"_\".join(file.split(\"_\")[:-1])\n                videos.setdefault(video_id, []).append(os.path.join(folder, file))\n\n            for frames in videos.values():\n                if len(frames) >= sequence_len:\n                    self.data.append(frames[:sequence_len])\n                    self.labels.append(label_idx)\n\n        print(f\"Loaded {len(self.data)} sequences from {root_dir}\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        frames = self.data[idx]\n        label = self.labels[idx]\n        images = [self.transform(Image.open(f).convert('RGB')) for f in frames]\n        return torch.stack(images), label\n\n# Define Image Transformations\ntransform = transforms.Compose([\n    transforms.Resize((299, 299)),\n    transforms.ToTensor(),\n])\n\n# Load Training & Validation Datasets\ntrain_ds = SeqImageDataset(f\"{seq_data_path}/train\", transform=transform)\nval_ds = SeqImageDataset(f\"{seq_data_path}/validation\", transform=transform)\n\n# Create DataLoaders\ntrain_dl = DataLoader(train_ds, batch_size=4, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=4)\n\n# Define Xception + GRU Model\nclass XceptionGRU(nn.Module):\n    def __init__(self, hidden_size=128):\n        super().__init__()\n        self.cnn = timm.create_model(\"xception\", pretrained=True, num_classes=0)  # Pretrained Xception\n        self.rnn = nn.GRU(input_size=2048, hidden_size=hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, 2)  # Binary classification: Real or Fake\n\n    def forward(self, x):\n        B, T, C, H, W = x.shape  # Batch, Time Steps, Channels, Height, Width\n        x = x.view(B * T, C, H, W)  # Flatten for CNN processing\n        feats = self.cnn(x)  # Extract CNN features (B*T, 2048)\n        feats = feats.view(B, T, -1)  # Reshape for GRU (Batch, Time, Features)\n        _, h = self.rnn(feats)  # GRU output\n        return self.fc(h[-1])  # Fully connected layer\n\n# Device Configuration (Use GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = XceptionGRU().to(device)\n\n# Define Optimizer and Loss Function\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\n# Train the Model\ndef train_model(model, train_dl, val_dl, epochs=5):\n    train_losses, val_losses = [], []\n    train_accuracies, val_accuracies = [], []\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        correct, total = 0, 0\n\n        for images, labels in train_dl:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n        train_acc = correct / total\n        train_losses.append(total_loss / len(train_dl))\n        train_accuracies.append(train_acc)\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        val_correct, val_total = 0, 0\n        with torch.no_grad():\n            for images, labels in val_dl:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                val_correct += (predicted == labels).sum().item()\n                val_total += labels.size(0)\n\n        val_acc = val_correct / val_total\n        val_losses.append(val_loss / len(val_dl))\n        val_accuracies.append(val_acc)\n\n        print(f\"Epoch {epoch+1}/{epochs}, \"\n              f\"Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, \"\n              f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n\n    return train_losses, val_losses, train_accuracies, val_accuracies\n\n# Validation metrics\ndef plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies):\n    plt.figure(figsize=(12, 5))\n\n    # Loss Plot\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label=\"Train Loss\")\n    plt.plot(val_losses, label=\"Val Loss\")\n    plt.title(\"Loss over Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n\n    # Accuracy Plot\n    plt.subplot(1, 2, 2)\n    plt.plot(train_accuracies, label=\"Train Accuracy\")\n    plt.plot(val_accuracies, label=\"Val Accuracy\")\n    plt.title(\"Accuracy over Epochs\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\ndef evaluate_model(model, val_dl):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in val_dl:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Classification report\n    from sklearn.utils.multiclass import unique_labels\n    labels_present = unique_labels(all_labels, all_preds)\n\n    class_names = [\"Real\", \"Fake\"]\n    class_labels = list(range(len(class_names)))\n\n    # Filter names to match existing labels\n    filtered_names = [class_names[i] for i in labels_present]\n\n    print(\"Classification Report:\\n\", classification_report(all_labels, all_preds, labels=labels_present, target_names=filtered_names))\n\n    print(\"Validation class counts:\", {label: all_labels.count(label) for label in set(all_labels)})\n\n    # Confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Real\", \"Fake\"], yticklabels=[\"Real\", \"Fake\"])\n    plt.title(\"Confusion Matrix\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.show()\n\n# Train and collect metrics\ntrain_losses, val_losses, train_accuracies, val_accuracies = train_model(model, train_dl, val_dl, epochs=5)\n\n# Save the model\ntorch.save(model, \"/content/drive/MyDrive/DeepFake/xception_gru_seq_model.pth\")\nprint(\"Model Saved Successfully!\")\n\n# Plot curves\nplot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies)\n\n# Evaluate performance\nevaluate_model(model, val_dl)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Building ensemble model for prediction using weighted average method","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\nimport cv2\nimport timm\nimport os\nimport zipfile\nimport shutil\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ==== Code 1: ViT Model ====\nmodel1 = torch.load(\n    \"/content/drive/MyDrive/DeepFake/deepfake_detector/vit_model.pth\",\n    map_location=device,\n    weights_only=False\n)\nmodel1.to(device)\nmodel1.eval()\n\ntransform1 = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\ndef predict_code1_image(image_path):\n    image = Image.open(image_path).convert('RGB')\n    image = transform1(image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        output = model1(image)\n        probabilities = F.softmax(output, dim=1)\n        confidence, predicted = torch.max(probabilities, 1)\n    class_names = ['Deepfake', 'Real']\n    prediction = class_names[predicted.item()]\n    return prediction\n\n# ==== Code 2: XceptionNet+GRU Model (for video data) ====\nclass XceptionGRU_Code2(nn.Module):\n    def __init__(self, hidden_size=256, num_classes=2):\n        super().__init__()\n        self.cnn = models.efficientnet_b0(pretrained=True)\n        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])\n        for param in self.cnn.parameters():\n            param.requires_grad = False\n        self.gru = nn.GRU(input_size=1280, hidden_size=hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        B, T, C, H, W = x.shape\n        x = x.view(B * T, C, H, W)\n        with torch.no_grad():\n            x = self.cnn(x)\n        x = x.view(B, T, -1)\n        _, h = self.gru(x)\n        return self.fc(h[-1])\n\nmodel2 = XceptionGRU_Code2().to(device)\nmodel2.load_state_dict(torch.load(\n    \"/content/drive/MyDrive/DeepFake/deepfake_detector/xception_gru_video_model.pth\",\n    map_location=device))\nmodel2.eval()\n\nvideo_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\ndef predict_code2_video(video_path, max_frames=16):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    count = 0\n    while count < max_frames and cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frame = Image.fromarray(frame)\n        frames.append(video_transform(frame))\n        count += 1\n    cap.release()\n    while len(frames) < max_frames:\n        frames.append(frames[-1])\n    input_tensor = torch.stack(frames).unsqueeze(0).to(device)\n    with torch.no_grad():\n        output = model2(input_tensor)\n        probabilities = torch.softmax(output, dim=1)\n        predicted_class = torch.argmax(probabilities, dim=1).item()\n    class_names = [\"Real\", \"Fake\"]\n    return class_names[predicted_class]\n\n# ==== Code 3: Xception+GRU Model (for sequential images data) ====\nclass XceptionGRU_Code3(nn.Module):\n    def __init__(self, hidden_size=128):\n        super().__init__()\n        self.cnn = timm.create_model(\"xception\", pretrained=True, num_classes=0)\n        self.rnn = nn.GRU(input_size=2048, hidden_size=hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, 2)\n\n    def forward(self, x):\n        B, T, C, H, W = x.shape\n        x = x.view(B * T, C, H, W)\n        feats = self.cnn(x)\n        feats = feats.view(B, T, -1)\n        _, h = self.rnn(feats)\n        return self.fc(h[-1])\n\nmodel3 = torch.load(\n    '/content/drive/MyDrive/DeepFake/deepfake_detector/xception_gru_seq_model.pth',\n    map_location=device,\n    weights_only=False\n)\nmodel3.eval()\n\ntransform3 = transforms.Compose([\n    transforms.Resize((299, 299)),\n    transforms.ToTensor(),\n])\n\ndef extract_frames(video_path, num_frames=10):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    while len(frames) < num_frames and cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frames.append(frame)\n    cap.release()\n    while len(frames) < num_frames:\n        frames.append(frames[-1])\n    return frames[:num_frames]\n\ndef preprocess_frames(frames):\n    transformed = []\n    for frame in frames:\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        img = Image.fromarray(frame_rgb)\n        img_t = transform3(img)\n        transformed.append(img_t)\n    return torch.stack(transformed)\n\ndef predict_code3_video(video_path):\n    frames = extract_frames(video_path)\n    input_tensor = preprocess_frames(frames).unsqueeze(0).to(device)\n    with torch.no_grad():\n        output = model3(input_tensor)\n    predicted_class = torch.argmax(output).item()\n    return 'Real' if predicted_class == 0 else 'Fake'\n\n# ==== Weighted Average ensemble ====\ndef biased_majority_vote(predictions):\n    weights = {'Code1': 1, 'Code2': 2, 'Code3': 0}\n    scores = {'Real': 0, 'Fake': 0}\n    for idx, pred in enumerate(predictions):\n        model_name = f'Code{idx+1}'\n        scores[pred] += weights[model_name]\n    final = max(scores, key=scores.get)\n    return final\n\n# ==== Process Input ====\ndef process_input(input_path):\n    if input_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n        print(\"Detected: Video Input\")\n        code2_result = predict_code2_video(input_path)\n        frames = extract_frames(input_path)\n        img = Image.fromarray(cv2.cvtColor(frames[0], cv2.COLOR_BGR2RGB))\n        image_tensor = transform1(img).unsqueeze(0).to(device)\n        with torch.no_grad():\n            output = model1(image_tensor)\n            pred = torch.argmax(F.softmax(output, dim=1)).item()\n        code1_result = 'Fake' if pred == 0 else 'Real'\n        code3_result = predict_code3_video(input_path)\n        predictions = [code1_result, code2_result, code3_result]\n    else:\n        print(\"Detected: Image Input\")\n        code1_result = predict_code1_image(input_path)\n        img = Image.open(input_path).convert('RGB')\n        frame_tensor = video_transform(img).unsqueeze(0).unsqueeze(0).to(device)\n        with torch.no_grad():\n            output = model2(frame_tensor)\n            pred = torch.argmax(torch.softmax(output, dim=1)).item()\n        code2_result = \"Real\" if pred == 0 else \"Fake\"\n        img_t = transform3(img).unsqueeze(0).unsqueeze(0).to(device)\n        with torch.no_grad():\n            output = model3(img_t)\n            pred = torch.argmax(output).item()\n        code3_result = 'Real' if pred == 0 else 'Fake'\n        predictions = ['Fake' if code1_result == 'Deepfake' else 'Real', code2_result, code3_result]\n\n    final = biased_majority_vote(predictions)\n    print(f\"\\nVotes: {predictions}\")\n    print(f\"Final Decision: {final}\")\n    return final\n\n# === UNZIP THE DATASET ===\nzip_path = \"/content/drive/MyDrive/DeepFake/Overall_Test.zip\"\nextract_to = \"/content/deepfake_data\"\n\nif os.path.exists(extract_to):\n    shutil.rmtree(extract_to)\n\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall(extract_to)\n\n# === COLLECT FILE PATHS ===\nreal_dir = os.path.join(extract_to, \"Overall_Test/Real\")\nfake_dir = os.path.join(extract_to, \"Overall_Test/Fake\")\n\nreal_paths = [os.path.join(real_dir, f) for f in os.listdir(real_dir) if f.endswith(('.jpg', '.png'))]\nfake_paths = [os.path.join(fake_dir, f) for f in os.listdir(fake_dir) if f.endswith(('.jpg', '.png'))]\n\n# === EVALUATE ===\ny_true = []\ny_pred = []\n\nprint(\"----- Predicting Real Images -----\\n\")\nfor path in real_paths:\n    pred = process_input(path)\n    y_true.append(\"Real\")\n    y_pred.append(pred)\n    print(f\"{os.path.basename(path)} â†’ Predicted: {pred} | Actual: Real\")\n\nprint(\"\\n----- Predicting Fake Images -----\\n\")\nfor path in fake_paths:\n    pred = process_input(path)\n    y_true.append(\"Fake\")\n    y_pred.append(pred)\n    print(f\"{os.path.basename(path)} â†’ Predicted: {pred} | Actual: Fake\")\n\n# === METRICS & VISUALIZATION ===\naccuracy = accuracy_score(y_true, y_pred)\nprint(f\"\\n\\nOverall Accuracy: {accuracy*100:.2f}%\")\nprint(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_true, y_pred, labels=[\"Real\", \"Fake\"])\nplt.figure(figsize=(6,5))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Real\", \"Fake\"], yticklabels=[\"Real\", \"Fake\"])\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n# Bar plot: Correct vs Incorrect\ncorrect = sum(1 for a, b in zip(y_true, y_pred) if a == b)\nincorrect = len(y_true) - correct\n\nplt.figure(figsize=(5,4))\nsns.barplot(x=[\"Correct\", \"Incorrect\"], y=[correct, incorrect], palette=\"Set2\")\nplt.title(\"Prediction Accuracy Summary\")\nplt.ylabel(\"Number of Images\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}